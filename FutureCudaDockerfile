FROM nvidia/cuda:13.1.0-devel-ubuntu24.04

# Install dependencies
RUN apt-get update && apt-get install -y \
    python3-pip \
    python3-dev \
    cmake \
    git \
    build-essential \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /code

ENV GGML_CUDA=on
ENV GGML_CUDA_ARCH="100;120"
ENV CUDA_DOCKER_ARCH=100
#ENV LIBRARY_PATH=/usr/local/cuda/lib64/stubs
#ENV LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH

RUN CMAKE_ARGS="-DGGML_CUDA=on -DGGML_CUDA_ARCH=${GGML_CUDA_ARCH}" \
    FORCE_CMAKE=1 \
    CMAKE_BUILD_PARALLEL_LEVEL=2 \
    pip install llama-cpp-python --no-cache-dir --break-system-packages \
        --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu125

# Install Python dependencies
RUN pip install "fastapi[standard]" "uvicorn[standard]" httpx --break-system-packages

COPY ./app /code

EXPOSE 8080

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8080"]
